{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is configs/config.py ###\n",
    "\n",
    "config_CELEBA = {\n",
    "    \"dataset_name\": \"CELEBA\",\n",
    "    \"image_shape\": (3, 64, 64),\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 500,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"project_name\": \"CELEBA_wandb\",\n",
    "    \"log_sample_interval\": 5,\n",
    "    \"wandb_name\": \"Test\",\n",
    "    \"data_dir\": \"cropped_celeba_bin\",  # Path to your .bin files\n",
    "\n",
    "}\n",
    "\n",
    "config_CIFAR10 = {\n",
    "    \"dataset_name\": \"CIFAR10\",\n",
    "    \"image_shape\": (3, 32, 32),  \n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 500,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"project_name\": \"CIFAR10_wandb\",\n",
    "    \"log_sample_interval\": 5,\n",
    "    \"wandb_name\": \"Test\",\n",
    "}\n",
    "\n",
    "config_MNIST = {\n",
    "    \"dataset_name\": \"MNIST\",\n",
    "    \"image_shape\": (1, 32, 32),\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 500,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"project_name\": \"MNIST_wandb\",\n",
    "    \"log_sample_interval\": 5,\n",
    "    \"wandb_name\": \"Test\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The is utils/dataset_loader.py ###\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import os\n",
    "\n",
    "def load_dataset(config, small_sample=False, validation=False):\n",
    "    validationSize = 0.1\n",
    "    if config[\"dataset_name\"] == \"MNIST\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Pad((2, 2, 2, 2), fill=0),  # Adds 2 pixels to each side with black padding\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        dataset = datasets.MNIST(\"mnist\", download=True, transform=transform)\n",
    "\n",
    "    elif config[\"dataset_name\"] == \"CIFAR10\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # RGB Normalize to [-1, 1]\n",
    "        ])\n",
    "        dataset = datasets.CIFAR10(\"cifar10\", download=True, transform=transform)\n",
    "\n",
    "    elif config[\"dataset_name\"] == \"CELEBA\":\n",
    "        # Define transformation\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config[\"image_shape\"][1], config[\"image_shape\"][2])),  \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize to [-1, 1]\n",
    "        ])\n",
    "\n",
    "        # Get all data batch files\n",
    "        data_batch_dir = \"cropped_celeba_bin\"\n",
    "        batch_files = [os.path.join(data_batch_dir, f) for f in os.listdir(data_batch_dir) if f.startswith(\"data_batch_\")]\n",
    "\n",
    "        # Load all binary files\n",
    "        dataset = CombinedBinaryDataset(bin_files=batch_files, img_size=(128, 128), num_channels=3, transform=transform)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {config['dataset_name']} is not supported!\")\n",
    "    \n",
    "    if small_sample:\n",
    "        # Use only the first 100 samples for quick testing\n",
    "        dataset = Subset(dataset, range(10))\n",
    "    \n",
    "    if validation:\n",
    "        dataset_size = len(dataset)\n",
    "        val_size = int(validationSize * dataset_size)\n",
    "        dataset = Subset(dataset, range(dataset_size - val_size, dataset_size))\n",
    "        print(\"Loading validation set\")\n",
    "    else:\n",
    "        dataset_size = len(dataset)\n",
    "        train_size = int((1-validationSize) * dataset_size)\n",
    "        dataset = Subset(dataset, range(0, train_size))\n",
    "        print(\"Loading training set\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class CombinedBinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, bin_files, img_size, num_channels=3, transform=None):\n",
    "        self.bin_files = bin_files  # List of binary file paths\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        # Read all batches into memory\n",
    "        for bin_file in self.bin_files:\n",
    "            file_size = os.path.getsize(bin_file)\n",
    "            sample_size = 1 + num_channels * img_size[0] * img_size[1]  # 1 byte label + pixel data\n",
    "            num_samples = file_size // sample_size\n",
    "\n",
    "            #print(f\"Loading {num_samples} samples from {bin_file}\")\n",
    "\n",
    "            with open(bin_file, \"rb\") as f:\n",
    "                for _ in range(num_samples):\n",
    "                    raw = f.read(sample_size)\n",
    "                    self.samples.append(raw)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw = self.samples[idx]\n",
    "        label = raw[0]  # Dummy label\n",
    "        pixels = torch.tensor(\n",
    "            list(raw[1:]), dtype=torch.float32\n",
    "        ).reshape(self.num_channels, *self.img_size) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            # Convert to PIL Image for compatibility with transforms\n",
    "            pixels = transforms.ToPILImage()(pixels)\n",
    "            pixels = self.transform(pixels)\n",
    "\n",
    "        return pixels, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the blocks used in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The is UNetResBlock/blocks.py ###\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Taken from https://github.com/dome272/Diffusion-Models-pytorch\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),  # Convolution\n",
    "            nn.GroupNorm(32, out_channels),  # Group normalization\n",
    "            nn.SiLU(),  # SiLU activation\n",
    "            nn.Dropout(p=0.1),\n",
    "        )\n",
    "        \n",
    "        # Optional 1x1 convolution for residual connection if dimensions mismatch\n",
    "        self.residual_conv = (\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.conv(x) + self.residual_conv(x)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "        )\n",
    "        \n",
    "        self.residual_conv = (\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "        self.time_proj = nn.Linear(time_dim, out_channels)  # Project time embedding\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        x_conv = self.conv(x)  # Standard conv operation\n",
    "        # Project time embedding to spatial dimensions\n",
    "        temb_proj = self.time_proj(temb)\n",
    "        temb_proj = temb_proj[:, :, None, None].repeat(1, 1, x_conv.shape[-2], x_conv.shape[-1])\n",
    "        x_output = x_conv + temb_proj\n",
    "        residual = self.residual_conv(x)\n",
    "        return self.activation(x_output + residual)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res_block_1 = ResBlock(in_channels, out_channels, time_dim)\n",
    "        self.downSample = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.res_block_2 = ResBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        x = self.res_block_1(x, temb)\n",
    "        x_down = self.downSample(x)\n",
    "        x_down = self.res_block_2(x_down, temb)\n",
    "        return x_down\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.upSample = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)\n",
    "        self.res_block_1 = ResBlock(in_channels * 2, in_channels, time_dim)\n",
    "        self.res_block_2 = ResBlock(in_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, x_skip, temb):\n",
    "        x_up = self.upSample(x)\n",
    "        x_cat = torch.cat([x_skip, x_up], dim=1)\n",
    "        x_up = self.res_block_1(x_cat, temb)\n",
    "        x_up = self.res_block_2(x_up, temb)\n",
    "        return x_up\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.dense2 = nn.Linear(projection_dim, projection_dim)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, t):\n",
    "        temb = self.dense1(t)\n",
    "        temb = self.activation(temb)\n",
    "        temb = self.dense2(temb)\n",
    "        return temb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for alpha and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The is UNetResBlock/alphabeta.py ###\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
    "    \"\"\"\n",
    "    Linear schedule for betas from beta_start to beta_end over timesteps.\n",
    "    \"\"\"\n",
    "    betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "    return betas\n",
    "\n",
    "def compute_alpha_schedule(betas):\n",
    "    \"\"\"\n",
    "    Computes alpha values for each timestep using the betas.\n",
    "    alpha_t = 1 - beta_t\n",
    "    \"\"\"\n",
    "    return 1.0 - betas\n",
    "\n",
    "def compute_alpha_cumulative_product(alpha_t):\n",
    "    \"\"\"\n",
    "    Computes the cumulative product of alpha_t over all timesteps.\n",
    "    This gives us the alpha_cumprod needed for the reverse process.\n",
    "    \"\"\"\n",
    "    return torch.cumprod(alpha_t, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The is UNetResBlock/EMA.py ###\n",
    "\n",
    "\n",
    "# Written by chatgpt, modified by us\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        \"\"\"\n",
    "        Exponential Moving Average (EMA) for model parameters.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to track.\n",
    "            decay (float): EMA decay factor, usually close to 1 (e.g., 0.999).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"\n",
    "        Update the EMA parameters with the current model parameters.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The current model with updated weights.\n",
    "        \"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name].data.mul_(self.decay).add_((1.0 - self.decay) * param.data)\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        \"\"\"Apply the EMA weights to the model.\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    def store(self):\n",
    "        \"\"\"Store the current model parameters for restoration.\"\"\"\n",
    "        self.backup = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"Restore the original model parameters.\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.data.copy_(self.backup[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The is UNetResBlock/model.py ###\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, dim=32, in_channels=3, out_channels=3, time_dim=256, device=\"cuda\"):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.time_embedding_dim = 128  # Or some other dimension\n",
    "        self.time_projection_dim = 512\n",
    "        self.time_embedder = TimeEmbedding(self.time_embedding_dim, self.time_projection_dim)\n",
    "\n",
    "        # Encoder (Downsampling path)\n",
    "        self.inc = ResBlock(in_channels, 64, self.time_projection_dim)  # (b, 64, 32, 32)\n",
    "        self.down1 = DownBlock(64, 128, self.time_projection_dim)       # (b, 128, 16, 16)\n",
    "        self.attn1 = SelfAttention(128, dim // 2)\n",
    "        self.down2 = DownBlock(128, 256, self.time_projection_dim)      # (b, 256, 8, 8)\n",
    "        self.attn2 = SelfAttention(256, dim // 4)\n",
    "        self.down3 = DownBlock(256, 512, self.time_projection_dim)      # (b, 512, 4, 4)\n",
    "        self.attn3 = SelfAttention(512, dim // 8)\n",
    "        self.down4 = DownBlock(512, 1024, self.time_projection_dim)     # (b, 1024, 2, 2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bot1 = ResBlock(1024, 1024, self.time_projection_dim)      # (b, 1024, 2, 2)\n",
    "        self.bot2 = ResBlock(1024, 1024, self.time_projection_dim)      # (b, 1024, 2, 2)\n",
    "        self.attn_bot = SelfAttention(1024, dim // 16)\n",
    "        self.bot3 = ResBlock(1024, 512, self.time_projection_dim)       # (b, 512, 2, 2)\n",
    "\n",
    "        # Decoder (Upsampling path)\n",
    "        self.up1 = UpBlock(512, 256, self.time_projection_dim)          # (b, 256, 4, 4)\n",
    "        self.attn4 = SelfAttention(256, dim // 8)\n",
    "        self.up2 = UpBlock(256, 128, self.time_projection_dim)          # (b, 128, 8, 8)\n",
    "        self.attn5 = SelfAttention(128, dim // 4)\n",
    "        self.up3 = UpBlock(128, 64, self.time_projection_dim)           # (b, 64, 16, 16)\n",
    "        self.attn6 = SelfAttention(64, dim // 2)\n",
    "        self.up4 = UpBlock(64, 64, self.time_projection_dim)            # (b, 64, 32, 32)\n",
    "\n",
    "        # Output layer\n",
    "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)          # Final output (b, c_out, 32, 32)\n",
    "\n",
    "    \n",
    "    def get_timestep_embedding(self, timesteps, embedding_dim):\n",
    "        \"\"\"\n",
    "        Sinusoidal embeddings for discrete timesteps.\n",
    "        \"\"\"\n",
    "        assert len(timesteps.shape) == 1, \"Timesteps should be a 1D tensor\"\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None].float() * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if embedding_dim % 2 == 1:\n",
    "            emb = torch.nn.functional.pad(emb, (0, 1))  # Zero pad to match dimensions\n",
    "        return emb\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.get_timestep_embedding(t, self.time_embedding_dim)\n",
    "        t_emb = self.time_embedder(t_emb)  # Project embedding\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.inc(x, t_emb)            # (b, 64, 32, 32)\n",
    "        x2 = self.down1(x1, t_emb)         # (b, 128, 16, 16)\n",
    "        x2 = self.attn1(x2)\n",
    "        x3 = self.down2(x2, t_emb)         # (b, 256, 8, 8)\n",
    "        x3 = self.attn2(x3)\n",
    "        x4 = self.down3(x3, t_emb)         # (b, 512, 4, 4)\n",
    "        x4 = self.attn3(x4)\n",
    "        x5 = self.down4(x4, t_emb)         # (b, 1024, 2, 2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bot1(x5, t_emb)           # (b, 1024, 2, 2)\n",
    "        x = self.bot2(x, t_emb)           # (b, 1024, 2, 2)\n",
    "        x = self.attn_bot(x)\n",
    "        x = self.bot3(x, t_emb)           # (b, 512, 2, 2)\n",
    "\n",
    "        # Decoder path (Upsampling)\n",
    "        x = self.up1(x, x4, t_emb)         # (b, 256, 4, 4)\n",
    "        x = self.attn4(x)\n",
    "        x = self.up2(x, x3, t_emb)         # (b, 128, 8, 8)\n",
    "        x = self.attn5(x)\n",
    "        x = self.up3(x, x2, t_emb)         # (b, 64, 16, 16)\n",
    "        x = self.attn6(x)\n",
    "        x = self.up4(x, x1, t_emb)         # (b, 64, 32, 32)\n",
    "\n",
    "        # Final output\n",
    "        output = self.outc(x)              # (b, c_out, 32, 32)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001  # Small noise variance at start\n",
    "    beta_end = 0.02  # Larger noise variance at end\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def compute_alpha_and_alpha_bar(betas):\n",
    "    alphas = 1.0 - betas\n",
    "    alpha_bar = torch.cumprod(alphas, dim=0)\n",
    "    return alphas, alpha_bar\n",
    "\n",
    "def calc_loss(u_net, x, timesteps=1000):\n",
    "    # Define the linear beta schedule\n",
    "    betas = linear_beta_schedule(timesteps)\n",
    "    _, alpha_bar = compute_alpha_and_alpha_bar(betas)\n",
    "    \n",
    "    # Sample random time steps\n",
    "    t = torch.randint(0, timesteps, (x.size(0),), device=x.device).long()\n",
    "    \n",
    "    # Get alpha_bar_t values for chosen time steps\n",
    "    alpha_bar_t = alpha_bar.to(x.device)[t].view(-1, 1, 1, 1)    \n",
    "    \n",
    "    # Add noise according to the forward process\n",
    "    noise = torch.randn_like(x)\n",
    "    x_t = torch.sqrt(alpha_bar_t) * x + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "    \n",
    "    # Predict the noise using the score network\n",
    "    predicted_noise = u_net(x_t, t/timesteps)\n",
    "    \n",
    "    # Compute the loss as the difference between predicted and actual noise\n",
    "    loss = torch.mean((predicted_noise - noise) ** 2)\n",
    "    return loss\n",
    "\n",
    "def generate_samples(u_net, nsamples, image_shape, timesteps=1000):\n",
    "    # Define the linear beta schedule\n",
    "    betas = linear_beta_schedule(timesteps)\n",
    "    alphas, alpha_bar = compute_alpha_and_alpha_bar(betas)\n",
    "\n",
    "    device = next(u_net.parameters()).device\n",
    "    x_t = torch.randn((nsamples, *image_shape), device=device)  # Start from pure noise\n",
    "    \n",
    "    for t in reversed(range(timesteps)):\n",
    "        t_tensor = torch.full((x_t.size(0),), t, device=device).long()  # Current time step\n",
    "        \n",
    "        # Compute the variance terms\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alpha_bar[t]\n",
    "        alpha_bar_prev = alpha_bar[t - 1] if t > 0 else 1.0\n",
    "        \n",
    "        # Predict the noise\n",
    "        predicted_noise = u_net(x_t, t_tensor/timesteps).detach()\n",
    "        \n",
    "        # Reconstruct the mean (mu) of x_{t-1}\n",
    "        mu = (1 / torch.sqrt(alpha_t)) * (\n",
    "            x_t - (betas[t] / torch.sqrt(1 - alpha_bar_t)) * predicted_noise\n",
    "        )\n",
    "        \n",
    "        # Add noise for all steps except the final one\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            sigma_t = torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar_t) * betas[t])\n",
    "            x_t = mu + sigma_t * noise\n",
    "        else:\n",
    "            x_t = mu  # No noise added at the final step\n",
    "    \n",
    "    return x_t\n",
    "\n",
    "\n",
    "def save_intermediate_images(images, timestep, save_dir = \"UNet/GenInt\"):\n",
    "    \"\"\"\n",
    "    Save a batch of intermediate images at a specific timestep.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    grid = vutils.make_grid(images, nrow=4, normalize=True, scale_each=True)\n",
    "    grid = (grid.permute(1, 2, 0).cpu().numpy() * 255).astype('uint8')  # Convert to uint8\n",
    "    img = Image.fromarray(grid)\n",
    "    img.save(os.path.join(save_dir, f\"timestep_{timestep:04d}.png\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    print(f\"[{tag}] Allocated Memory: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "    print(f\"[{tag}] Reserved Memory: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The is UNetResBlock/trainer.py ###\n",
    "\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(u_net, dataset, config, model_name, log=False, save_model=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    u_net = u_net.to(device)\n",
    "    ema = EMA(u_net, decay=0.995)  # Initialize EMA tracker\n",
    "\n",
    "    image_shape = config[\"image_shape\"]\n",
    "    save_dir = os.path.join(\"UNetResBlock/results\", model_name)  # Create the full directory path\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Optimizer\n",
    "    opt = torch.optim.Adam(u_net.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    # DataLoader\n",
    "    dloader = torch.utils.data.DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "\n",
    "    p_bar = False\n",
    "\n",
    "    for i_epoch in range(config[\"epochs\"]):\n",
    "        print(f\"Epoch {i_epoch} started, EMA\")\n",
    "        total_loss = 0\n",
    "\n",
    "        # Wrap DataLoader with or without tqdm for progress bar\n",
    "        if p_bar:  # Default to True if not specified\n",
    "            num_batches = len(dloader)\n",
    "            progress_bar = tqdm(dloader, total=num_batches, desc=f\"Epoch {i_epoch}\", ncols=100)\n",
    "        else:\n",
    "            progress_bar = dloader  # Use the dataloader directly without tqdm\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, (data, _) in enumerate(progress_bar):\n",
    "            data = data.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = calc_loss(u_net, data)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            # Update EMA after optimizer step\n",
    "            ema.update(u_net)\n",
    "\n",
    "            total_loss += loss.item() * data.shape[0]\n",
    "\n",
    "            # Update the description of the progress bar with current loss\n",
    "            if p_bar:  # Only update tqdm if it's enabled\n",
    "                progress_bar.set_postfix(loss=loss.item(), total_loss=total_loss)\n",
    "\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        if log:\n",
    "            wandb.log({\"loss\": avg_loss}, step=i_epoch)\n",
    "\n",
    "        # Save the model with EMA weights\n",
    "        # Save the model with EMA weights\n",
    "        if save_model:\n",
    "            ema.store()  # Save the current model parameters\n",
    "            ema.apply_shadow()  # Apply EMA weights to the model\n",
    "            torch.save(u_net.state_dict(), f\"UNetResBlock/models/{model_name}_ema.pt\")\n",
    "            ema.restore()  # Restore original weights for further training\n",
    "\n",
    "        # Generate samples using EMA weights\n",
    "        if (i_epoch < 5 or i_epoch % 5 == 0):\n",
    "            print(\"Generating samples with EMA weights...\")\n",
    "            ema.store()  # Save the current model parameters\n",
    "            ema.apply_shadow()  # Apply EMA weights\n",
    "            generated_samples = generate_samples(u_net, nsamples=16, image_shape=image_shape, timesteps=1000)\n",
    "            ema.restore()  # Restore original weights for further training\n",
    "            \n",
    "            # Save the generated samples as a row in the specified folder\n",
    "            save_samples(generated_samples, save_dir, f\"epoch{i_epoch}\", i_epoch, wandb_log=True)\n",
    "            save_samples(generated_samples, \"/zhome/1a/a/156609/public_html/ShowResults\", \"resultOwn\")\n",
    "            \n",
    "            # Save the first 8 images from the dataset\n",
    "            first_batch, _ = next(iter(torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False)))\n",
    "            save_samples(first_batch.to(device), save_dir, f\"dataset_samples\")\n",
    "            \n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        print(f\"Epoch {i_epoch}, Loss: {avg_loss}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "def save_samples(samples, save_dir, filename, i_epoch=0, wandb_log=False):\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Move the samples to CPU if on GPU and scale from [-1, 1] to [0, 1]\n",
    "    samples = samples.cpu()\n",
    "    # Compute statistics\n",
    "    min_val = samples.min().item()\n",
    "    max_val = samples.max().item()\n",
    "    avg_val = samples.mean().item()\n",
    "    std_val = samples.std().item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"file name: {filename}\")\n",
    "    print(f\"with samples min: {min_val:.2f}, samples max: {max_val:.2f}, avg: {avg_val:.2f}, std: {std_val:.2f}\")\n",
    "\n",
    "    samples = (samples + 1) / 2  # Scale from [-1, 1] to [0, 1]\n",
    "    samples = samples.clamp(0, 1)  # Ensure values are within [0, 1]\n",
    "\n",
    "    # Save concatenated image as a single PNG file\n",
    "    images = []\n",
    "    for i in range(samples.shape[0]):\n",
    "        img = samples[i].permute(1, 2, 0).clamp(0, 1).numpy() * 255\n",
    "        if samples.shape[1] == 1:  # Grayscale\n",
    "            img = img[:, :, 0]\n",
    "            img = Image.fromarray(img.astype('uint8'), mode='L')\n",
    "        elif samples.shape[1] == 3:  # RGB\n",
    "            img = Image.fromarray(img.astype('uint8'))\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected number of channels: {samples.shape[1]}\")\n",
    "        images.append(img)\n",
    "\n",
    "    concatenated_image = Image.new(\n",
    "        'RGB' if samples.shape[1] == 3 else 'L',\n",
    "        (samples.shape[0] * images[0].width, images[0].height)\n",
    "    )\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        concatenated_image.paste(img, (i * img.width, 0))\n",
    "\n",
    "    # Save the concatenated image\n",
    "    save_path = os.path.join(save_dir, f\"{filename}.png\")\n",
    "    concatenated_image.save(save_path)\n",
    "\n",
    "    # Optionally log to wandb with matplotlib for grayscale\n",
    "    if wandb_log:\n",
    "        grid_size = 4  # 4x4 grid\n",
    "        num_images = samples.shape[0]\n",
    "\n",
    "        if samples.shape[1] == 1:  # Grayscale images\n",
    "            fig, axes = plt.subplots(grid_size, grid_size, figsize=(8, 8))\n",
    "            axes = axes.flatten()  # Flatten the 2D grid of axes for easy iteration\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                if i < num_images:\n",
    "                    img = samples[i][0].cpu().numpy()  # Take the grayscale channel\n",
    "                    ax.imshow(1 - img, cmap=\"Greys\")\n",
    "                ax.axis(\"off\")  # Turn off axes for all plots, including blanks\n",
    "\n",
    "            wandb.log({f\"GeneratedSamples:\": wandb.Image(fig)}, step = i_epoch)\n",
    "            plt.close(fig)\n",
    "        else:  # RGB images\n",
    "            fig, axes = plt.subplots(grid_size, grid_size, figsize=(8, 8))\n",
    "            axes = axes.flatten()  # Flatten the 2D grid of axes for easy iteration\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                if i < num_images:\n",
    "                    img = samples[i].permute(1, 2, 0).cpu().numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "                    ax.imshow((img * 255).astype(\"uint8\"))  # Ensure RGB is properly scaled\n",
    "                ax.axis(\"off\")  # Turn off axes for all plots, including blanks\n",
    "\n",
    "            wandb.log({f\"GeneratedSamples:\": wandb.Image(fig)}, step = i_epoch)\n",
    "            plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.10.12)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/zhome/1a/a/156609/project/path/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "### The is mainUNetResBlock.py ###\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "from utils.dataset_loader import load_dataset\n",
    "from configs.config import config_MNIST, config_CIFAR10, config_CELEBA\n",
    "from UNetResBlock.model import UNet\n",
    "from UNetResBlock.trainer import train_model\n",
    "import random\n",
    "\n",
    "config = config_CELEBA\n",
    "\n",
    "configDSName = config[\"dataset_name\"]\n",
    "name = f\"{configDSName}_{random.randint(100000, 1000000)}\"\n",
    "print (f\"starting run: {name}\")\n",
    "\n",
    "# Initialize W&B\n",
    "logwandb = True\n",
    "\n",
    "# Use only small subset of data (for debugging)\n",
    "debugDataSize = False\n",
    "modelNameTest = \"_smallDS\" if debugDataSize else \"\"\n",
    "\n",
    "save_model = True\n",
    "model_name = \"ResNet\"+name+modelNameTest # File that the model is saved as. Only relevant if save_model = True\n",
    "\n",
    "\n",
    "if logwandb: \n",
    "    wandb.init(project=config[\"project_name\"], name=model_name, config=config)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(config,small_sample=debugDataSize)\n",
    "\n",
    "channels = config[\"image_shape\"][0]\n",
    "dim = config[\"image_shape\"][1]\n",
    "\n",
    "# Initialize model\n",
    "model = UNet(in_channels=channels, dim = config[\"image_shape\"][1], out_channels=channels)\n",
    "\n",
    "# Train model\n",
    "train_model(model, dataset, config, model_name=model_name, log=logwandb, save_model = save_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make binary files used for FID scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is generateAndRead/save_model_binary.py ###\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the module to the Python path\n",
    "external_module_path = \"/zhome/1a/a/156609/project/path\"\n",
    "sys.path.append(external_module_path)\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Configurations\n",
    "config = config_CELEBA\n",
    "model_filename = \"ResNetCELEBA_518406_ema.pt\"\n",
    "batch_size = 200  # Number of samples to generate per batch\n",
    "num_samples = 10000  # Number of samples to generate\n",
    "\n",
    "# idk\n",
    "image_shape = config[\"image_shape\"]  # Channels, Height, Width\n",
    "\n",
    "# Define the output folder and ensure it exists\n",
    "output_folder = \"generateAndRead/binSamples\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "dataset_name = config[\"dataset_name\"]\n",
    "output_binary_file = os.path.join(output_folder, f\"model_{dataset_name}_{num_samples}samples_ema.bin\")\n",
    "\n",
    "# Load the saved model\n",
    "model_folder = \"savedModels\"\n",
    "model_path = os.path.join(model_folder, model_filename)\n",
    "\n",
    "\n",
    "channels = config[\"image_shape\"][0]\n",
    "dim = config[\"image_shape\"][1]\n",
    "# Initialize and load the model\n",
    "u_net = UNet(in_channels=channels, dim = config[\"image_shape\"][1], out_channels=channels)\n",
    "state_dict = torch.load(model_path, weights_only=True)\n",
    "u_net.load_state_dict(state_dict)\n",
    "u_net.eval()\n",
    "\n",
    "# Generate samples in parallel batches\n",
    "print(\"Generating samples in parallel...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "u_net.to(device)\n",
    "\n",
    "# Adjust the shape of `all_samples` to include channels\n",
    "all_samples = np.zeros((num_samples, *image_shape), dtype=np.uint8)\n",
    "\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    current_batch_size = min(batch_size, num_samples - i)\n",
    "    print(f\"Generating batch {i // batch_size + 1}: {current_batch_size} samples...\")\n",
    "    \n",
    "    # Generate samples for the current batch\n",
    "    samples = generate_samples(u_net, current_batch_size, image_shape).detach().cpu().numpy()\n",
    "    \n",
    "    # Normalize the samples\n",
    "    samples = (samples - samples.min(axis=(1, 2, 3), keepdims=True)) / \\\n",
    "              (samples.max(axis=(1, 2, 3), keepdims=True) - samples.min(axis=(1, 2, 3), keepdims=True)) * 255\n",
    "    samples = samples.astype(np.uint8)\n",
    "\n",
    "    # Store samples directly without squeezing the channel dimension\n",
    "    all_samples[i:i + current_batch_size] = samples\n",
    "\n",
    "# Save all samples to a binary file\n",
    "with open(output_binary_file, \"wb\") as f:\n",
    "    f.write(all_samples.tobytes())\n",
    "\n",
    "print(f\"All samples saved to {output_binary_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfhub\n",
    "import tensorflow_gan as tfgan\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the pre-trained MNIST classifier\n",
    "MNIST_MODULE = \"https://tfhub.dev/tensorflow/tfgan/eval/mnist/logits/1\"\n",
    "mnist_classifier_fn = tfhub.load(MNIST_MODULE)\n",
    "\n",
    "IMAGE_MODULE = \"https://www.kaggle.com/models/tensorflow/inception/tensorFlow1/tfgan-eval-inception/1\"\n",
    "image_classifier_fn = tfhub.load(IMAGE_MODULE)\n",
    "\n",
    "def wrapped_image_classifier_fn(input_tensor):\n",
    "    # Ensure input_tensor has 3 channels (convert grayscale to RGB)\n",
    "    if input_tensor.shape[-1] == 1:  # Check if it's grayscale\n",
    "        input_tensor = tf.image.grayscale_to_rgb(input_tensor)\n",
    "\n",
    "    # Resize images to 299x299 for Inception\n",
    "    # input_tensor = tf.image.resize(input_tensor, [299, 299])\n",
    "\n",
    "    # Pass the processed input to the classifier\n",
    "    output = image_classifier_fn(input_tensor)\n",
    "    \n",
    "    return output['pool_3']  # Use 'pool_3' for FID, or adjust based on requirements\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_activations(tensors, num_batches, classifier_fn):\n",
    "    \"\"\"\n",
    "    Given a tensor of of shape (batch_size, height, width, channels), computes\n",
    "    the activiations given by classifier_fn.\n",
    "    \"\"\"\n",
    "    tensors_list = tf.split(tensors, num_or_size_splits=num_batches)\n",
    "    stack = tf.stack(tensors_list)\n",
    "    activation = tf.nest.map_structure(\n",
    "        tf.stop_gradient,\n",
    "        tf.map_fn(classifier_fn, stack, parallel_iterations=1, swap_memory=True),\n",
    "    )\n",
    "    return tf.concat(tf.unstack(activation), 0)\n",
    "\n",
    "\n",
    "\n",
    "def read_binary_file(file_path, image_shape, isValidation):\n",
    "    \"\"\"\n",
    "    Reads a binary file containing labeled images.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the binary file.\n",
    "        image_shape (tuple): Shape of each image (channels, height, width).\n",
    "        num_images (int): Number of images to read.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of images of shape (num_images, channels, height, width).\n",
    "    \"\"\"\n",
    "    # Bytes per image: 1 byte for label + image data\n",
    "    if isValidation:\n",
    "        bytes_per_image = np.prod(image_shape) + 1\n",
    "    else:\n",
    "        bytes_per_image =  np.prod(image_shape)\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "    total_bytes = os.path.getsize(file_path)\n",
    "\n",
    "    num_images =  total_bytes // bytes_per_image - 1\n",
    "\n",
    "    images = []\n",
    "    for i in range(num_images):\n",
    "        start = i * bytes_per_image + 1  # Skip the label\n",
    "        end = start + np.prod(image_shape)\n",
    "        image = data[start:end].reshape(image_shape)\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "def dataset_to_numpy(config, validation=False):\n",
    "    \"\"\"\n",
    "    Load a dataset and convert it directly to a NumPy array of unnormalized images.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary specifying dataset parameters.\n",
    "        validation (bool): Whether to load the validation set.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of images of shape (num_images, channels, height, width).\n",
    "    \"\"\"\n",
    "    from utils.dataset_loader import load_dataset\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(config, validation=validation)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Function to unnormalize images from [-1, 1] to [0, 255]\n",
    "    def unnormalize(images):\n",
    "        return ((images * 0.5) + 0.5) * 255\n",
    "\n",
    "    images = []\n",
    "    # Iterate over the DataLoader to fetch images\n",
    "    for image, _ in dataloader:\n",
    "        # Unnormalize and convert to numpy\n",
    "        image = unnormalize(image).squeeze(0).byte().numpy()  # Remove batch dimension, scale, and convert\n",
    "        images.append(image)\n",
    "\n",
    "    # Convert list of numpy arrays to a single numpy array\n",
    "    return np.stack(images)\n",
    "\n",
    "def compute_fid_for_CIFAR_or_CELEBA(dataset, image_shape, validation_load_data):\n",
    "    \"\"\"\n",
    "    Computes FID for CIFAR10 or CELEBA dataset using TensorFlow GAN utilities.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Name of the dataset ('CIFAR10' or 'CELEBA').\n",
    "        image_shape (tuple): Shape of each image (channels, height, width).\n",
    "        batch_size (int): Number of images to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        float: Computed FID score.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    if dataset == \"CIFAR10\":\n",
    "        generated_path = \"generateAndRead/binSamples/model_CIFAR10_10000samples.bin\"\n",
    "        validation_path = \"generateAndRead/binSamples/CIFAR10_validation_samples.bin\"\n",
    "        config = config_CIFAR10\n",
    "    elif dataset == \"CELEBA\":\n",
    "        generated_path = \"generateAndRead/binSamples/model_CELEBA_10000samples_int.bin\"\n",
    "        validation_path = \"generateAndRead/binSamples/CELEBA_validation_samples.bin\"\n",
    "        config = config_CELEBA\n",
    "    elif dataset == \"MNIST\":\n",
    "        generated_path = \"generateAndRead/binSamples/model_MNIST_10000samples.bin\"\n",
    "        validation_path = \"generateAndRead/binSamples/MNIST_validation_samples.bin\"\n",
    "        config = config_MNIST\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "    output_dir = \"./output\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Load validation images\n",
    "    logger.info(f\"Reading validation images from binary file...\")\n",
    "    # validation_images = read_binary_file(validation_path, image_shape, isValidation=True)\n",
    "    validation_images = dataset_to_numpy(config, validation=validation_load_data)\n",
    "    validation_images = validation_images / 255.0  # Normalize to [0, 1]\n",
    "    validation_images_tf = tf.convert_to_tensor(validation_images, dtype=tf.float32)\n",
    "    validation_images_tf = tf.transpose(validation_images_tf, [0, 2, 3, 1])  # Convert to NHWC format\n",
    "\n",
    "    # Load generated images\n",
    "    logger.info(f\"Reading generated images from binary file...\")\n",
    "    generated_images = read_binary_file(generated_path, image_shape, isValidation=False)\n",
    "    generated_images = generated_images / 255.0  # Normalize to [0, 1]\n",
    "    generated_images_tf = tf.convert_to_tensor(generated_images, dtype=tf.float32)\n",
    "    generated_images_tf = tf.transpose(generated_images_tf, [0, 2, 3, 1])  # Convert to NHWC format\n",
    "\n",
    "    # Compute activations for validation images\n",
    "    logger.info(\"Computing activations for validation images...\")\n",
    "    activations_real = compute_activations(validation_images_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)\n",
    "\n",
    "    # Compute activations for generated images\n",
    "    logger.info(\"Computing activations for generated images...\")\n",
    "    activations_fake = compute_activations(generated_images_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)\n",
    "\n",
    "    # Compute FID\n",
    "    logger.info(\"Computing FID score...\")\n",
    "\n",
    "    # Reshape activations to rank-2 tensors\n",
    "    activations_fake = tf.reshape(activations_fake, [activations_fake.shape[0], activations_fake.shape[-1]])\n",
    "    activations_real = tf.reshape(activations_real, [activations_real.shape[0], activations_real.shape[-1]])\n",
    "\n",
    "    # Compute FID\n",
    "    fid_score = tfgan.eval.frechet_classifier_distance_from_activations(activations_real, activations_fake)\n",
    "\n",
    "    logger.info(f\"FID score: {fid_score}\")\n",
    "\n",
    "    # Save activations and results\n",
    "    np.save(os.path.join(output_dir, \"activations_fake.npy\"), activations_fake.numpy())\n",
    "    np.save(os.path.join(output_dir, \"activations_real.npy\"), activations_real.numpy())\n",
    "\n",
    "    print(f\"Computed FID for {dataset}, val comparison {validation_load_data}: {fid_score.numpy()}\")\n",
    "    return fid_score.numpy()\n",
    "\n",
    "\n",
    "\n",
    "def compute_fid_for_CIFAR_or_CELEBA_batch(dataset, image_shape, validation_load_data, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Computes FID for CIFAR10 or CELEBA dataset using TensorFlow GAN utilities.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Name of the dataset ('CIFAR10' or 'CELEBA').\n",
    "        image_shape (tuple): Shape of each image (channels, height, width).\n",
    "        validation_load_data (bool): Indicates whether to load validation data.\n",
    "        batch_size (int): Number of images to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        float: Computed FID score.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    paths = {\n",
    "        \"CIFAR10\": (\"model_CIFAR10_10000samples.bin\", \"CIFAR10_validation_samples.bin\", config_CIFAR10),\n",
    "        \"CELEBA\": (\"model_CELEBA_10000samples_ema.bin\", \"CELEBA_validation_samples.bin\", config_CELEBA),\n",
    "        \"MNIST\": (\"model_MNIST_10000samples.bin\", \"MNIST_validation_samples.bin\", config_MNIST)\n",
    "    }\n",
    "    \n",
    "    if dataset not in paths:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "    \n",
    "    generated_file, validation_file, config = paths[dataset]\n",
    "    generated_path = os.path.join(\"generateAndRead/binSamples\", generated_file)\n",
    "    validation_path = os.path.join(\"generateAndRead/binSamples\", validation_file)\n",
    "    \n",
    "    output_dir = \"./output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Load validation images\n",
    "    logger.info(\"Reading and processing validation images in batches...\")\n",
    "    validation_images = dataset_to_numpy(config, validation=validation_load_data)\n",
    "    validation_images = validation_images.astype(np.float32) / 255.0  # Normalize and ensure dtype is float32\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices(validation_images).batch(batch_size)\n",
    "\n",
    "    activations_real_list = []\n",
    "    for batch_index, batch in enumerate(validation_dataset):\n",
    "        logger.info(f\"Processing batch {batch_index + 1}/{len(validation_dataset)}...\")\n",
    "        batch_tf = tf.transpose(batch, [0, 2, 3, 1])  # Convert to NHWC format\n",
    "        activations_batch = compute_activations(batch_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)\n",
    "        activations_real_list.append(activations_batch)\n",
    "        \n",
    "    activations_real = tf.concat(activations_real_list, axis=0)\n",
    "    activations_real = tf.reshape(activations_real, [activations_real.shape[0], -1])  # Reshape to rank-2 tensor\n",
    "\n",
    "    # Load generated images\n",
    "    logger.info(\"Reading generated images...\")\n",
    "    generated_images = read_binary_file(generated_path, image_shape, isValidation=False)\n",
    "    generated_images = generated_images.astype(np.float32) / 255.0  # Normalize and ensure dtype is float32\n",
    "    generated_images_tf = tf.convert_to_tensor(generated_images, dtype=tf.float32)\n",
    "    generated_images_tf = tf.transpose(generated_images_tf, [0, 2, 3, 1])  # Convert to NHWC format\n",
    "\n",
    "    # Compute activations for generated images\n",
    "    logger.info(\"Computing activations for generated images...\")\n",
    "    activations_fake = compute_activations(generated_images_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)\n",
    "    activations_fake = tf.reshape(activations_fake, [activations_fake.shape[0], -1])  # Reshape to rank-2 tensor\n",
    "\n",
    "    # Compute FID\n",
    "    logger.info(\"Computing FID score...\")\n",
    "    fid_score = tfgan.eval.frechet_classifier_distance_from_activations(activations_real, activations_fake)\n",
    "\n",
    "    logger.info(f\"FID score: {fid_score}\")\n",
    "\n",
    "    # Save activations and results\n",
    "    np.save(os.path.join(output_dir, \"activations_fake.npy\"), activations_fake.numpy())\n",
    "    np.save(os.path.join(output_dir, \"activations_real.npy\"), activations_real.numpy())\n",
    "\n",
    "    print(f\"Computed FID for {dataset}, validation comparison {validation_load_data}: {fid_score.numpy()}\")\n",
    "    return fid_score.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage, can also be made with MNIST, though might not be as accurate.\n",
    "fid = compute_fid_for_CIFAR_or_CELEBA_batch(\n",
    "    dataset=\"CIFAR10\",\n",
    "    image_shape=(3, 32, 32),\n",
    "    validation_load_data = False,\n",
    ")\n",
    "\n",
    "fid = compute_fid_for_CIFAR_or_CELEBA(\n",
    "    dataset=\"CELEBA\",\n",
    "    image_shape=(3, 64, 64),\n",
    "    validation_load_data = True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
