# -*- coding: utf-8 -*-
"""FID scorer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xUJML29cjxFWm60Ai05BB8iYgN9CY3sq
"""

# Install the required versions of TensorFlow, TensorFlow Probability, and TensorFlow GAN
#!pip install tensorflow==2.15 tensorflow-probability==0.23 tensorflow-gan matplotlib tqdm

# Import necessary libraries
import tensorflow as tf
import tensorflow_hub as tfhub
import tensorflow_gan as tfgan
import numpy as np
from torchvision import datasets, transforms
import torch
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import logging
import os
from configs.config import config_CIFAR10, config_CELEBA, config_MNIST


# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load the pre-trained MNIST classifier
MNIST_MODULE = "https://tfhub.dev/tensorflow/tfgan/eval/mnist/logits/1"
mnist_classifier_fn = tfhub.load(MNIST_MODULE)

IMAGE_MODULE = "https://www.kaggle.com/models/tensorflow/inception/tensorFlow1/tfgan-eval-inception/1"
image_classifier_fn = tfhub.load(IMAGE_MODULE)

def wrapped_image_classifier_fn(input_tensor):
    # Ensure input_tensor has 3 channels (convert grayscale to RGB)
    if input_tensor.shape[-1] == 1:  # Check if it's grayscale
        input_tensor = tf.image.grayscale_to_rgb(input_tensor)

    # Resize images to 299x299 for Inception
    # input_tensor = tf.image.resize(input_tensor, [299, 299])

    # Pass the processed input to the classifier
    output = image_classifier_fn(input_tensor)
    
    return output['pool_3']  # Use 'pool_3' for FID, or adjust based on requirements




def compute_activations(tensors, num_batches, classifier_fn):
    """
    Given a tensor of of shape (batch_size, height, width, channels), computes
    the activiations given by classifier_fn.
    """
    tensors_list = tf.split(tensors, num_or_size_splits=num_batches)
    stack = tf.stack(tensors_list)
    activation = tf.nest.map_structure(
        tf.stop_gradient,
        tf.map_fn(classifier_fn, stack, parallel_iterations=1, swap_memory=True),
    )
    return tf.concat(tf.unstack(activation), 0)



def read_binary_file(file_path, image_shape, isValidation):
    """
    Reads a binary file containing labeled images.

    Args:
        file_path (str): Path to the binary file.
        image_shape (tuple): Shape of each image (channels, height, width).
        num_images (int): Number of images to read.

    Returns:
        np.ndarray: Array of images of shape (num_images, channels, height, width).
    """
    # Bytes per image: 1 byte for label + image data
    if isValidation:
        bytes_per_image = np.prod(image_shape) + 1
    else:
        bytes_per_image =  np.prod(image_shape)

    with open(file_path, "rb") as f:
        data = np.frombuffer(f.read(), dtype=np.uint8)

    total_bytes = os.path.getsize(file_path)

    num_images =  total_bytes // bytes_per_image - 1

    images = []
    for i in range(num_images):
        start = i * bytes_per_image + 1  # Skip the label
        end = start + np.prod(image_shape)
        image = data[start:end].reshape(image_shape)
        images.append(image)
    return np.array(images)

def dataset_to_numpy(config, validation=False):
    """
    Load a dataset and convert it directly to a NumPy array of unnormalized images.

    Args:
        config (dict): Configuration dictionary specifying dataset parameters.
        validation (bool): Whether to load the validation set.

    Returns:
        np.ndarray: Array of images of shape (num_images, channels, height, width).
    """
    from utils.dataset_loader import load_dataset
    # Load the dataset
    dataset = load_dataset(config, validation=validation)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)

    # Function to unnormalize images from [-1, 1] to [0, 255]
    def unnormalize(images):
        return ((images * 0.5) + 0.5) * 255

    images = []
    # Iterate over the DataLoader to fetch images
    for image, _ in dataloader:
        # Unnormalize and convert to numpy
        image = unnormalize(image).squeeze(0).byte().numpy()  # Remove batch dimension, scale, and convert
        images.append(image)

    # Convert list of numpy arrays to a single numpy array
    return np.stack(images)

def compute_fid_for_CIFAR_or_CELEBA(dataset, image_shape, validation_load_data):
    """
    Computes FID for CIFAR10 or CELEBA dataset using TensorFlow GAN utilities.

    Args:
        dataset (str): Name of the dataset ('CIFAR10' or 'CELEBA').
        image_shape (tuple): Shape of each image (channels, height, width).
        batch_size (int): Number of images to process in each batch.

    Returns:
        float: Computed FID score.
    """
    # File paths
    if dataset == "CIFAR10":
        generated_path = "generateAndRead/binSamples/model_CIFAR10_10000samples.bin"
        validation_path = "generateAndRead/binSamples/CIFAR10_validation_samples.bin"
        config = config_CIFAR10
    elif dataset == "CELEBA":
        generated_path = "generateAndRead/binSamples/model_CELEBA_10000samples_int.bin"
        validation_path = "generateAndRead/binSamples/CELEBA_validation_samples.bin"
        config = config_CELEBA
    elif dataset == "MNIST":
        generated_path = "generateAndRead/binSamples/model_MNIST_10000samples.bin"
        validation_path = "generateAndRead/binSamples/MNIST_validation_samples.bin"
        config = config_MNIST
    else:
        raise ValueError(f"Unknown dataset: {dataset}")

    tf.debugging.set_log_device_placement(True)

    output_dir = "./output"

    os.makedirs(output_dir, exist_ok=True)


    # Load validation images
    logger.info(f"Reading validation images from binary file...")
    # validation_images = read_binary_file(validation_path, image_shape, isValidation=True)
    validation_images = dataset_to_numpy(config, validation=validation_load_data)
    validation_images = validation_images / 255.0  # Normalize to [0, 1]
    validation_images_tf = tf.convert_to_tensor(validation_images, dtype=tf.float32)
    validation_images_tf = tf.transpose(validation_images_tf, [0, 2, 3, 1])  # Convert to NHWC format

    # Load generated images
    logger.info(f"Reading generated images from binary file...")
    generated_images = read_binary_file(generated_path, image_shape, isValidation=False)
    generated_images = generated_images / 255.0  # Normalize to [0, 1]
    generated_images_tf = tf.convert_to_tensor(generated_images, dtype=tf.float32)
    generated_images_tf = tf.transpose(generated_images_tf, [0, 2, 3, 1])  # Convert to NHWC format

    # Compute activations for validation images
    logger.info("Computing activations for validation images...")
    activations_real = compute_activations(validation_images_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)

    # Compute activations for generated images
    logger.info("Computing activations for generated images...")
    activations_fake = compute_activations(generated_images_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)

    # Compute FID
    logger.info("Computing FID score...")

    # Reshape activations to rank-2 tensors
    activations_fake = tf.reshape(activations_fake, [activations_fake.shape[0], activations_fake.shape[-1]])
    activations_real = tf.reshape(activations_real, [activations_real.shape[0], activations_real.shape[-1]])

    # Compute FID
    fid_score = tfgan.eval.frechet_classifier_distance_from_activations(activations_real, activations_fake)

    logger.info(f"FID score: {fid_score}")

    # Save activations and results
    np.save(os.path.join(output_dir, "activations_fake.npy"), activations_fake.numpy())
    np.save(os.path.join(output_dir, "activations_real.npy"), activations_real.numpy())

    print(f"Computed FID for {dataset}, val comparison {validation_load_data}: {fid_score.numpy()}")
    return fid_score.numpy()



def compute_fid_for_CIFAR_or_CELEBA_batch(dataset, image_shape, validation_load_data, batch_size=10000):
    """
    Computes FID for CIFAR10 or CELEBA dataset using TensorFlow GAN utilities.

    Args:
        dataset (str): Name of the dataset ('CIFAR10' or 'CELEBA').
        image_shape (tuple): Shape of each image (channels, height, width).
        validation_load_data (bool): Indicates whether to load validation data.
        batch_size (int): Number of images to process in each batch.

    Returns:
        float: Computed FID score.
    """
    # Define file paths
    paths = {
        "CIFAR10": ("model_CIFAR10_10000samples.bin", "CIFAR10_validation_samples.bin", config_CIFAR10),
        "CELEBA": ("model_CELEBA_10000samples_ema.bin", "CELEBA_validation_samples.bin", config_CELEBA),
        "MNIST": ("model_MNIST_10000samples.bin", "MNIST_validation_samples.bin", config_MNIST)
    }
    
    if dataset not in paths:
        raise ValueError(f"Unknown dataset: {dataset}")
    
    generated_file, validation_file, config = paths[dataset]
    generated_path = os.path.join("generateAndRead/binSamples", generated_file)
    validation_path = os.path.join("generateAndRead/binSamples", validation_file)
    
    output_dir = "./output"
    os.makedirs(output_dir, exist_ok=True)

    # Setup logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    # Load validation images
    logger.info("Reading and processing validation images in batches...")
    validation_images = dataset_to_numpy(config, validation=validation_load_data)
    validation_images = validation_images.astype(np.float32) / 255.0  # Normalize and ensure dtype is float32
    validation_dataset = tf.data.Dataset.from_tensor_slices(validation_images).batch(batch_size)

    activations_real_list = []
    for batch_index, batch in enumerate(validation_dataset):
        logger.info(f"Processing batch {batch_index + 1}/{len(validation_dataset)}...")
        batch_tf = tf.transpose(batch, [0, 2, 3, 1])  # Convert to NHWC format
        activations_batch = compute_activations(batch_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)
        activations_real_list.append(activations_batch)
        
    activations_real = tf.concat(activations_real_list, axis=0)
    activations_real = tf.reshape(activations_real, [activations_real.shape[0], -1])  # Reshape to rank-2 tensor

    # Load generated images
    logger.info("Reading generated images...")
    generated_images = read_binary_file(generated_path, image_shape, isValidation=False)
    generated_images = generated_images.astype(np.float32) / 255.0  # Normalize and ensure dtype is float32
    generated_images_tf = tf.convert_to_tensor(generated_images, dtype=tf.float32)
    generated_images_tf = tf.transpose(generated_images_tf, [0, 2, 3, 1])  # Convert to NHWC format

    # Compute activations for generated images
    logger.info("Computing activations for generated images...")
    activations_fake = compute_activations(generated_images_tf, num_batches=1, classifier_fn=wrapped_image_classifier_fn)
    activations_fake = tf.reshape(activations_fake, [activations_fake.shape[0], -1])  # Reshape to rank-2 tensor

    # Compute FID
    logger.info("Computing FID score...")
    fid_score = tfgan.eval.frechet_classifier_distance_from_activations(activations_real, activations_fake)

    logger.info(f"FID score: {fid_score}")

    # Save activations and results
    np.save(os.path.join(output_dir, "activations_fake.npy"), activations_fake.numpy())
    np.save(os.path.join(output_dir, "activations_real.npy"), activations_real.numpy())

    print(f"Computed FID for {dataset}, validation comparison {validation_load_data}: {fid_score.numpy()}")
    return fid_score.numpy()



# Example usage
fid = compute_fid_for_CIFAR_or_CELEBA_batch(
    dataset="CIFAR10",
    image_shape=(3, 32, 32),
    validation_load_data = False,
)

# fid = compute_fid_for_CIFAR_or_CELEBA(
#     dataset="CELEBA",
#     image_shape=(3, 64, 64),
#     validation_load_data = True,
# )




