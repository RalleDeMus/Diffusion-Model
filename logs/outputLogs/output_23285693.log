Generating samples in parallel...
Generating batch 1: 500 samples...
Generating batch 2: 500 samples...
Generating batch 3: 500 samples...
Generating batch 4: 500 samples...
Generating batch 5: 500 samples...
Generating batch 6: 500 samples...
Generating batch 7: 500 samples...
Generating batch 8: 500 samples...
Generating batch 9: 500 samples...
Generating batch 10: 500 samples...
Generating batch 11: 500 samples...
Generating batch 12: 500 samples...
Generating batch 13: 500 samples...
Generating batch 14: 500 samples...
Generating batch 15: 500 samples...
Generating batch 16: 500 samples...
Generating batch 17: 500 samples...
Generating batch 18: 500 samples...
Generating batch 19: 500 samples...
Generating batch 20: 500 samples...
All samples saved to generateAndRead/binSamples/model_MNIST_10000samples.bin.

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23285693: <mnist_training> in cluster <dcc> Done

Job <mnist_training> was submitted from host <n-62-12-19> by user <s204427> in cluster <dcc> at Thu Nov 28 15:26:46 2024
Job was executed on host(s) <4*n-62-18-9>, in queue <gpua100>, as user <s204427> in cluster <dcc> at Thu Nov 28 15:43:53 2024
</zhome/1a/a/156609> was used as the home directory.
</zhome/1a/a/156609/project/path> was used as the working directory.
Started at Thu Nov 28 15:43:53 2024
Terminated at Thu Nov 28 16:00:06 2024
Results reported at Thu Nov 28 16:00:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
# mnist_job.sh
#!/bin/bash
#BSUB -J mnist_training           # Job name
#BSUB -q gpua100                   # Queue name for Tesla A10 GPUs
#BSUB -gpu "num=1"                # Request 1 GPU in exclusive mode
#BSUB -n 4                        # Request 4 CPU cores (required)
#BSUB -R "span[hosts=1]"          # Ensure resources are on a single node
#BSUB -W 48:00                    # Walltime (1 hour)
#BSUB -R "rusage[mem=4096]"       # Request 4GB of system memory
#BSUB -o ./logs/outputLogs/output_%J.log            # Output file
#BSUB -e ./logs/errorLogs/error_%J.log             # Error file

# Load necessary modules
module load python3/3.10.12
module load cuda/12.1

# Set W&B API key (replace with your actual key)
export WANDB_API_KEY="6ecda4c80f57815b4ff4780014d596e19617454c"

# Activate virtual environment
source /zhome/1a/a/156609/project/path/.venv/bin/activate

# Run the PyTorch training script
python3 generateAndRead/save_model_binary.py 

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1437.00 sec.
    Max Memory :                                 689 MB
    Average Memory :                             633.25 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               15695.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   974 sec.
    Turnaround time :                            2000 sec.

The output (if any) is above this job summary.



PS:

Read file <./logs/errorLogs/error_23285693.log> for stderr output of this job.

