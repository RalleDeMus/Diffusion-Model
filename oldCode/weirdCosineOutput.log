Files already downloaded and verified
Image shape: torch.Size([3, 32, 32])
Epoch 0, Loss: 0.2478711265730858
Epoch 5, Loss: 0.038088375266194345
Epoch 10, Loss: 0.03235736069798469
Epoch 15, Loss: 0.029699481992721558
Epoch 20, Loss: 0.02815102661013603
Epoch 25, Loss: 0.02692545071899891
Epoch 30, Loss: 0.026484246190190314
Epoch 35, Loss: 0.026059821144342424
Epoch 40, Loss: 0.025499426743984224
Epoch 45, Loss: 0.024943829640746117
Epoch 50, Loss: 0.024507216517925264
Epoch 55, Loss: 0.02438829230070114
Epoch 60, Loss: 0.02379740720629692
Epoch 65, Loss: 0.02362168118298054
Epoch 70, Loss: 0.023257833000421523
Epoch 75, Loss: 0.023134232619404792
Epoch 80, Loss: 0.022693593378067017
Epoch 85, Loss: 0.022018261857628823
Epoch 90, Loss: 0.021521024503707884
Epoch 95, Loss: 0.02112434031844139
Epoch 100, Loss: 0.020478416036367415
Epoch 105, Loss: 0.020012132109999657
Epoch 110, Loss: 0.019802212234139444
Epoch 115, Loss: 0.019645282393097877
Epoch 120, Loss: 0.019339835152626036
Epoch 125, Loss: 0.01888842672109604
Epoch 130, Loss: 0.018640411968827247
Epoch 135, Loss: 0.018349721301198007
Epoch 140, Loss: 0.017909182918071746
Epoch 145, Loss: 0.017432104595899583
Epoch 150, Loss: 0.01695838316977024
Epoch 155, Loss: 0.01654973861217499
Epoch 160, Loss: 0.016065251463651658
Epoch 165, Loss: 0.015750753822922705
Epoch 170, Loss: 0.015604973918795586
Epoch 175, Loss: 0.015155960046350956
Epoch 180, Loss: 0.014993628163039684
Epoch 185, Loss: 0.014864231328964233
Epoch 190, Loss: 0.01463590950280428
Epoch 195, Loss: 0.014467792428135872
Epoch 200, Loss: 0.014287621986865997
Epoch 205, Loss: 0.014114213615059852
Epoch 210, Loss: 0.014050583876371384
Epoch 215, Loss: 0.013763215734064579
Epoch 220, Loss: 0.013689120912849902
Epoch 225, Loss: 0.013664938730597495
Epoch 230, Loss: 0.013381399165093899
Epoch 235, Loss: 0.013372714615166188
Epoch 240, Loss: 0.013408064016103744
Epoch 245, Loss: 0.013166840693056583
Epoch 250, Loss: 0.013090876513421536
Epoch 255, Loss: 0.012904236270785331
Epoch 260, Loss: 0.012925679111480712
Epoch 265, Loss: 0.01274679490685463
Epoch 270, Loss: 0.012812138636410236
Epoch 275, Loss: 0.012634062243103981
Epoch 280, Loss: 0.012718846019506454
Epoch 285, Loss: 0.012694625072777271
Epoch 290, Loss: 0.012565036639869213
Epoch 295, Loss: 0.012461892785429954
Epoch 300, Loss: 0.012425477479398251
Epoch 305, Loss: 0.012554994511306285
Epoch 310, Loss: 0.012633783750832081
Epoch 315, Loss: 0.01230574752509594
Epoch 320, Loss: 0.01233604879975319
Epoch 325, Loss: 0.01228198058873415
Epoch 330, Loss: 0.012230754058659078
Epoch 335, Loss: 0.012220325186252594
Epoch 340, Loss: 0.01215555121332407
Epoch 345, Loss: 0.012022040230631829
Epoch 350, Loss: 0.012101623780429364
Epoch 355, Loss: 0.012105302041769028
Epoch 360, Loss: 0.012016470737159252
Epoch 365, Loss: 0.01180117007791996
Epoch 370, Loss: 0.011742413592934608
Epoch 375, Loss: 0.011791105165779591
Epoch 380, Loss: 0.011803732746839523
Epoch 385, Loss: 0.01191424151301384
Epoch 390, Loss: 0.011761742010116576
Epoch 395, Loss: 0.011692133832275867
Epoch 400, Loss: 0.011610748983323574
Epoch 405, Loss: 0.01169901261240244
Epoch 410, Loss: 0.011560940218567848
Epoch 415, Loss: 0.011353364903330804
Epoch 420, Loss: 0.011659795745313168
Epoch 425, Loss: 0.01154036940574646
Epoch 430, Loss: 0.011462275476753712
Epoch 435, Loss: 0.011509112718105317
Epoch 440, Loss: 0.011560183829069138
Epoch 445, Loss: 0.011488162914812566
Epoch 450, Loss: 0.011429461409151555
Epoch 455, Loss: 0.011383323932886123
Epoch 460, Loss: 0.011277716050446033
Epoch 465, Loss: 0.011241762982606887
Epoch 470, Loss: 0.011485483073294164
Epoch 475, Loss: 0.011250981072485447
Epoch 480, Loss: 0.01135489180445671
Epoch 485, Loss: 0.01116003290683031
Epoch 490, Loss: 0.011136275005340576
Epoch 495, Loss: 0.01121768054485321
Epoch 500, Loss: 0.011386290996074676
Epoch 505, Loss: 0.011081858160495757
Epoch 510, Loss: 0.011226030975580215
Epoch 515, Loss: 0.011166251141428947
Epoch 520, Loss: 0.011142176240384579
Epoch 525, Loss: 0.011247975961863995
Epoch 530, Loss: 0.011055785047113895
Epoch 535, Loss: 0.01099560028463602
Epoch 540, Loss: 0.011006343650519848
Epoch 545, Loss: 0.01097100394308567
Epoch 550, Loss: 0.011091135024130344
Epoch 555, Loss: 0.010891598762869836
Epoch 560, Loss: 0.01093635270923376
Epoch 565, Loss: 0.010928737978935241
Epoch 570, Loss: 0.01095047204107046
Epoch 575, Loss: 0.011013122905492783
Epoch 580, Loss: 0.011056146632432937
Epoch 585, Loss: 0.010985731126666069
Epoch 590, Loss: 0.010800984430015087
Epoch 595, Loss: 0.010782346796691418
Epoch 600, Loss: 0.010903916828334331
Epoch 605, Loss: 0.010831148148477077
Epoch 610, Loss: 0.010732428677976132
Epoch 615, Loss: 0.010760507654547691
Epoch 620, Loss: 0.010794802931845189
Epoch 625, Loss: 0.01083423944145441
Epoch 630, Loss: 0.010793710858225822
Epoch 635, Loss: 0.01078198112666607
Epoch 640, Loss: 0.010693565488755703
Epoch 645, Loss: 0.01064053236991167
Epoch 650, Loss: 0.01085566939085722
Epoch 655, Loss: 0.010756278757750988
Epoch 660, Loss: 0.010713787762522697
Epoch 665, Loss: 0.010736310822963715
Epoch 670, Loss: 0.01084437660008669
Epoch 675, Loss: 0.010687386910915374
Epoch 680, Loss: 0.010576483627855777
Epoch 685, Loss: 0.010659502577781678
Epoch 690, Loss: 0.01055542755573988
Epoch 695, Loss: 0.010644570664316415
Epoch 700, Loss: 0.010672733168303966
Epoch 705, Loss: 0.01063511559844017
Epoch 710, Loss: 0.010660738201141358
Epoch 715, Loss: 0.010423661653995515
Epoch 720, Loss: 0.010480481477677822
Epoch 725, Loss: 0.010654224407672882
Epoch 730, Loss: 0.010480461584925651
Epoch 735, Loss: 0.010446021934747696
Epoch 740, Loss: 0.01053670480877161
Epoch 745, Loss: 0.010627526773810387
Epoch 750, Loss: 0.010470773268043994
Epoch 755, Loss: 0.010418666089773179
Epoch 760, Loss: 0.010586257936656475
Epoch 765, Loss: 0.010431344128847123
Epoch 770, Loss: 0.010345609170794487
Epoch 775, Loss: 0.010432469505369663
Epoch 780, Loss: 0.010435822432935237
Epoch 785, Loss: 0.010342717350125312
Epoch 790, Loss: 0.010447652829289436
Epoch 795, Loss: 0.010290397955477237
Epoch 800, Loss: 0.01046380831450224
Epoch 805, Loss: 0.010262548211067915
Epoch 810, Loss: 0.010285777815580368
Epoch 815, Loss: 0.010268113678693772
Epoch 820, Loss: 0.010378951382040978
Epoch 825, Loss: 0.01029683649480343
Epoch 830, Loss: 0.010308851686716079
Epoch 835, Loss: 0.010338853448629379
Epoch 840, Loss: 0.010402230542600154
Epoch 845, Loss: 0.010441646355986595
Epoch 850, Loss: 0.010230460819602013
Epoch 855, Loss: 0.010306389920413495
Epoch 860, Loss: 0.010182839498221874
Epoch 865, Loss: 0.01009739800453186
Epoch 870, Loss: 0.010277375680208206
Epoch 875, Loss: 0.010365716117620468
Epoch 880, Loss: 0.01022836106210947
Epoch 885, Loss: 0.010193489569425583
Epoch 890, Loss: 0.010263229674398898
Epoch 895, Loss: 0.010254066267311573
Epoch 900, Loss: 0.01021881924033165
Epoch 905, Loss: 0.010256468968391419
Epoch 910, Loss: 0.01024818839430809
Epoch 915, Loss: 0.010212731368243695
Epoch 920, Loss: 0.01012262392103672
Epoch 925, Loss: 0.010410480155050755
Epoch 930, Loss: 0.010232642369121314
Epoch 935, Loss: 0.01019295997351408
Epoch 940, Loss: 0.010086518886685372
Epoch 945, Loss: 0.010133487172424793
Epoch 950, Loss: 0.010136166725456714
Epoch 955, Loss: 0.010232885925769805
Epoch 960, Loss: 0.01004350252687931
Epoch 965, Loss: 0.00997873687416315
Epoch 970, Loss: 0.010198895636200906
Epoch 975, Loss: 0.010181714273691177
Epoch 980, Loss: 0.010095679150223733
Epoch 985, Loss: 0.010072893530428409
Epoch 990, Loss: 0.010168104506283998
Epoch 995, Loss: 0.009991233855783939

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23171190: <mnist_training> in cluster <dcc> Done

Job <mnist_training> was submitted from host <hpclogin1> by user <s204427> in cluster <dcc> at Thu Nov 14 18:38:46 2024
Job was executed on host(s) <4*n-62-12-23>, in queue <gpua100>, as user <s204427> in cluster <dcc> at Thu Nov 14 18:49:15 2024
</zhome/1a/a/156609> was used as the home directory.
</zhome/1a/a/156609/project/path> was used as the working directory.
Started at Thu Nov 14 18:49:15 2024
Terminated at Fri Nov 15 03:08:06 2024
Results reported at Fri Nov 15 03:08:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
# mnist_job.sh
#!/bin/bash
#BSUB -J mnist_training           # Job name
#BSUB -q gpua100                   # Queue name for Tesla A10 GPUs
#BSUB -gpu "num=1"                # Request 1 GPU in exclusive mode
#BSUB -n 4                        # Request 4 CPU cores (required)
#BSUB -R "span[hosts=1]"          # Ensure resources are on a single node
#BSUB -W 48:00                    # Walltime (1 hour)
#BSUB -R "rusage[mem=4096]"       # Request 4GB of system memory
#BSUB -o output_%J.log            # Output file
#BSUB -e error_%J.log             # Error file

# Load necessary modules
module load python3/3.10.12
module load cuda/12.1

# Set W&B API key (replace with your actual key)
export WANDB_API_KEY="6ecda4c80f57815b4ff4780014d596e19617454c"

# Activate virtual environment
source /zhome/1a/a/156609/project/path/.venv/bin/activate

# Run the PyTorch training script
python3 model1Cosine.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   29961.00 sec.
    Max Memory :                                 1214 MB
    Average Memory :                             1183.67 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               15170.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                26
    Run time :                                   29931 sec.
    Turnaround time :                            30560 sec.

The output (if any) is above this job summary.



PS:

Read file <error_23171190.log> for stderr output of this job.

