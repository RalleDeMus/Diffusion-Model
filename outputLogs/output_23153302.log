Image shape: torch.Size([1, 28, 28])
Epoch 0, Loss: 0.2859132147709529
Epoch 5, Loss: 0.04512983332475026
Epoch 10, Loss: 0.03995892359217008
Epoch 15, Loss: 0.03278603028059006
Epoch 20, Loss: 0.030999053979913392
Epoch 25, Loss: 0.0316725781361262
Epoch 30, Loss: 0.029049804921944936
Epoch 35, Loss: 0.02718064748843511
Epoch 40, Loss: 0.026780725319186845
Epoch 45, Loss: 0.02631627902587255
Epoch 50, Loss: 0.025431223396460215
Epoch 55, Loss: 0.024624298501014708
Epoch 60, Loss: 0.024361676505208017
Epoch 65, Loss: 0.02401437950929006
Epoch 70, Loss: 0.023902799200018247
Epoch 75, Loss: 0.02258848238984744
Epoch 80, Loss: 0.022750937802592915
Epoch 85, Loss: 0.02211092997988065
Epoch 90, Loss: 0.021469557583332063
Epoch 95, Loss: 0.020888335084915163
Epoch 100, Loss: 0.020678538165489833
Epoch 105, Loss: 0.020282342288891475
Epoch 110, Loss: 0.020763605038324993
Epoch 115, Loss: 0.01936422581076622
Epoch 120, Loss: 0.0198107812444369
Epoch 125, Loss: 0.01936461168130239
Epoch 130, Loss: 0.019298320670922596
Epoch 135, Loss: 0.01863476228415966
Epoch 140, Loss: 0.01858168933093548
Epoch 145, Loss: 0.017582133477926254
Epoch 150, Loss: 0.018187539477149645
Epoch 155, Loss: 0.017961526807149252
Epoch 160, Loss: 0.017617050896088282
Epoch 165, Loss: 0.01748561312953631
Epoch 170, Loss: 0.017480152547359466
Epoch 175, Loss: 0.017539760106801985
Epoch 180, Loss: 0.017474178571502366
Epoch 185, Loss: 0.018417057029406228
Epoch 190, Loss: 0.01718598763048649
Epoch 195, Loss: 0.016810811930894853
Epoch 200, Loss: 0.017069096793731052
Epoch 205, Loss: 0.016622242734829585
Epoch 210, Loss: 0.016738078359762827
Epoch 215, Loss: 0.01701780240436395
Epoch 220, Loss: 0.0162824547658364
Epoch 225, Loss: 0.016877295130491257
Epoch 230, Loss: 0.016233581604560215
Epoch 235, Loss: 0.016456845717628798
Epoch 240, Loss: 0.016537208555142086
Epoch 245, Loss: 0.01619176730712255
Epoch 250, Loss: 0.016195894661049048
Epoch 255, Loss: 0.015885224389036497
Epoch 260, Loss: 0.016097708187003932
Epoch 265, Loss: 0.015832983057697612
Epoch 270, Loss: 0.01584678383966287
Epoch 275, Loss: 0.015924221702416738
Epoch 280, Loss: 0.015926274208227793
Epoch 285, Loss: 0.015983144250512123
Epoch 290, Loss: 0.015605084590117137
Epoch 295, Loss: 0.015697687260309855
Epoch 300, Loss: 0.01594033955136935
Epoch 305, Loss: 0.015574259180823962
Epoch 310, Loss: 0.015935069704055786
Epoch 315, Loss: 0.015643132558465004
Epoch 320, Loss: 0.015390541113913058
Epoch 325, Loss: 0.015616039662063121
Epoch 330, Loss: 0.015335530022283396
Epoch 335, Loss: 0.0156542527337869
Epoch 340, Loss: 0.015216852256655693
Epoch 345, Loss: 0.015257491178313891
Epoch 350, Loss: 0.015274989583094915
Epoch 355, Loss: 0.015518454745908579
Epoch 360, Loss: 0.015520472458004952
Epoch 365, Loss: 0.015367115752895674
Epoch 370, Loss: 0.015543930857380232
Epoch 375, Loss: 0.015292267916103204
Epoch 380, Loss: 0.015333863071600597
Epoch 385, Loss: 0.015169456933935484
Epoch 390, Loss: 0.015141470424830914
Epoch 395, Loss: 0.015082748590906461
Epoch 400, Loss: 0.015060919172565142
Epoch 405, Loss: 0.015133798401554426
Epoch 410, Loss: 0.015046822665135066
Epoch 415, Loss: 0.01479044479727745
Epoch 420, Loss: 0.01493379858036836
Epoch 425, Loss: 0.015594662071267763
Epoch 430, Loss: 0.015283720624446868
Epoch 435, Loss: 0.014961447647213936
Epoch 440, Loss: 0.015249217996994654
Epoch 445, Loss: 0.015074003584682942
Epoch 450, Loss: 0.015005030503372352
Epoch 455, Loss: 0.014990318474670252
Epoch 460, Loss: 0.014882675839960575
Epoch 465, Loss: 0.01490260072449843
Epoch 470, Loss: 0.015046669036646684
Epoch 475, Loss: 0.0148741281285882
Epoch 480, Loss: 0.015061009137829144
Epoch 485, Loss: 0.01491657040665547
Epoch 490, Loss: 0.014860465800762177
Epoch 495, Loss: 0.01499544790784518

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23153302: <mnist_training> in cluster <dcc> Done

Job <mnist_training> was submitted from host <hpclogin1> by user <s204427> in cluster <dcc> at Tue Nov 12 16:37:54 2024
Job was executed on host(s) <4*n-62-18-9>, in queue <gpua100>, as user <s204427> in cluster <dcc> at Tue Nov 12 19:08:39 2024
</zhome/1a/a/156609> was used as the home directory.
</zhome/1a/a/156609/project/path> was used as the working directory.
Started at Tue Nov 12 19:08:39 2024
Terminated at Tue Nov 12 21:53:43 2024
Results reported at Tue Nov 12 21:53:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
# mnist_job.sh
#!/bin/bash
#BSUB -J mnist_training           # Job name
#BSUB -q gpua100                   # Queue name for Tesla A10 GPUs
#BSUB -gpu "num=1"                # Request 1 GPU in exclusive mode
#BSUB -n 4                        # Request 4 CPU cores (required)
#BSUB -R "span[hosts=1]"          # Ensure resources are on a single node
#BSUB -W 48:00                    # Walltime (1 hour)
#BSUB -R "rusage[mem=4096]"       # Request 4GB of system memory
#BSUB -o output_%J.log            # Output file
#BSUB -e error_%J.log             # Error file

# Load necessary modules
module load python3/3.10.12
module load cuda/12.1

# Set W&B API key (replace with your actual key)
export WANDB_API_KEY="6ecda4c80f57815b4ff4780014d596e19617454c"

# Activate virtual environment
source /zhome/1a/a/156609/project/path/.venv/bin/activate

# Run the PyTorch training script
python3 model1Cifar.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   9931.00 sec.
    Max Memory :                                 888 MB
    Average Memory :                             874.94 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               15496.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                26
    Run time :                                   9905 sec.
    Turnaround time :                            18949 sec.

The output (if any) is above this job summary.



PS:

Read file <error_23153302.log> for stderr output of this job.

