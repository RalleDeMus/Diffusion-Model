Files already downloaded and verified
Image shape: torch.Size([3, 32, 32])
Epoch 0, Loss: 0.4261894687652588
Epoch 5, Loss: 0.09329610768318176
Epoch 10, Loss: 0.05879493938922882
Epoch 15, Loss: 0.048785770930051806
Epoch 20, Loss: 0.043220437796115875
Epoch 25, Loss: 0.039622629753351214
Epoch 30, Loss: 0.03795498329401016
Epoch 35, Loss: 0.03344027239561081
Epoch 40, Loss: 0.032599351833462714
Epoch 45, Loss: 0.030580542422533035
Epoch 50, Loss: 0.03128516471028328
Epoch 55, Loss: 0.030059187598228454
Epoch 60, Loss: 0.02907938403069973
Epoch 65, Loss: 0.029644649345874786
Epoch 70, Loss: 0.02851670534670353
Epoch 75, Loss: 0.02845055098772049
Epoch 80, Loss: 0.02787535469532013
Epoch 85, Loss: 0.027193670684099197
Epoch 90, Loss: 0.02720507349550724
Epoch 95, Loss: 0.026836740681529046
Epoch 100, Loss: 0.026730375974178315
Epoch 105, Loss: 0.026304382202625275
Epoch 110, Loss: 0.02589443675518036
Epoch 115, Loss: 0.026036196627616883
Epoch 120, Loss: 0.02482339006304741
Epoch 125, Loss: 0.025259345877170564
Epoch 130, Loss: 0.024410634672939777
Epoch 135, Loss: 0.02427911248922348
Epoch 140, Loss: 0.024636244189143182
Epoch 145, Loss: 0.023870649368166925
Epoch 150, Loss: 0.026750559180378913
Epoch 155, Loss: 0.02396128345489502
Epoch 160, Loss: 0.02375212649345398
Epoch 165, Loss: 0.02355439480185509
Epoch 170, Loss: 0.023415346309244633
Epoch 175, Loss: 0.02326325596690178
Epoch 180, Loss: 0.023579957481622695
Epoch 185, Loss: 0.02339408112168312
Epoch 190, Loss: 0.02263756775736809
Epoch 195, Loss: 0.02265912816822529
Epoch 200, Loss: 0.022829602450728417
Epoch 205, Loss: 0.02262095602631569
Epoch 210, Loss: 0.022283990067839623
Epoch 215, Loss: 0.021972605177164078
Epoch 220, Loss: 0.022205161501765252
Epoch 225, Loss: 0.0223417904818058
Epoch 230, Loss: 0.022375376717448235
Epoch 235, Loss: 0.02184489092350006
Epoch 240, Loss: 0.022424287748336793
Epoch 245, Loss: 0.022280285788178443
Epoch 250, Loss: 0.021625726217031477
Epoch 255, Loss: 0.022573731044232846
Epoch 260, Loss: 0.021701739805936815
Epoch 265, Loss: 0.02165407918483019
Epoch 270, Loss: 0.022137801782488822
Epoch 275, Loss: 0.02115700623393059
Epoch 280, Loss: 0.02128860351741314
Epoch 285, Loss: 0.021584071118831635
Epoch 290, Loss: 0.02170828997015953
Epoch 295, Loss: 0.02114045759499073
Epoch 300, Loss: 0.021402758299708367
Epoch 305, Loss: 0.021803933873176574
Epoch 310, Loss: 0.021732352031171322
Epoch 315, Loss: 0.02159877674818039
Epoch 320, Loss: 0.02116643053561449
Epoch 325, Loss: 0.021316233511567115
Epoch 330, Loss: 0.020859254108667374
Epoch 335, Loss: 0.021147309646010398
Epoch 340, Loss: 0.021096764993071556
Epoch 345, Loss: 0.02070306049644947
Epoch 350, Loss: 0.020699845103025437
Epoch 355, Loss: 0.02082296733021736
Epoch 360, Loss: 0.021200238657593726
Epoch 365, Loss: 0.020585752167105675
Epoch 370, Loss: 0.02081707170844078
Epoch 375, Loss: 0.020857103608846666
Epoch 380, Loss: 0.02051197843968868
Epoch 385, Loss: 0.02115482766032219
Epoch 390, Loss: 0.020936056650280952
Epoch 395, Loss: 0.020539775699973107
Epoch 400, Loss: 0.02042180833220482
Epoch 405, Loss: 0.020376051148176193
Epoch 410, Loss: 0.020761190312504767
Epoch 415, Loss: 0.020638390148282053
Epoch 420, Loss: 0.0206951241594553
Epoch 425, Loss: 0.02060392679929733
Epoch 430, Loss: 0.02054467567563057
Epoch 435, Loss: 0.020137452422976494
Epoch 440, Loss: 0.020458910607099534
Epoch 445, Loss: 0.020331195414066316
Epoch 450, Loss: 0.02012911139279604
Epoch 455, Loss: 0.020248409342169763
Epoch 460, Loss: 0.020158585765361788
Epoch 465, Loss: 0.02072289665132761
Epoch 470, Loss: 0.020585728273391724
Epoch 475, Loss: 0.01982695048093796
Epoch 480, Loss: 0.020277210711240768
Epoch 485, Loss: 0.01998643393814564
Epoch 490, Loss: 0.020059093237519265
Epoch 495, Loss: 0.02068247612297535

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23153405: <mnist_training> in cluster <dcc> Done

Job <mnist_training> was submitted from host <hpclogin1> by user <s204427> in cluster <dcc> at Tue Nov 12 17:15:37 2024
Job was executed on host(s) <4*n-62-12-21>, in queue <gpua100>, as user <s204427> in cluster <dcc> at Tue Nov 12 19:48:25 2024
</zhome/1a/a/156609> was used as the home directory.
</zhome/1a/a/156609/project/path> was used as the working directory.
Started at Tue Nov 12 19:48:25 2024
Terminated at Tue Nov 12 23:11:41 2024
Results reported at Tue Nov 12 23:11:41 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
# mnist_job.sh
#!/bin/bash
#BSUB -J mnist_training           # Job name
#BSUB -q gpua100                   # Queue name for Tesla A10 GPUs
#BSUB -gpu "num=1"                # Request 1 GPU in exclusive mode
#BSUB -n 4                        # Request 4 CPU cores (required)
#BSUB -R "span[hosts=1]"          # Ensure resources are on a single node
#BSUB -W 48:00                    # Walltime (1 hour)
#BSUB -R "rusage[mem=4096]"       # Request 4GB of system memory
#BSUB -o output_%J.log            # Output file
#BSUB -e error_%J.log             # Error file

# Load necessary modules
module load python3/3.10.12
module load cuda/12.1

# Set W&B API key (replace with your actual key)
export WANDB_API_KEY="6ecda4c80f57815b4ff4780014d596e19617454c"

# Activate virtual environment
source /zhome/1a/a/156609/project/path/.venv/bin/activate

# Run the PyTorch training script
python3 model1Cifar10.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   12211.00 sec.
    Max Memory :                                 1185 MB
    Average Memory :                             1155.14 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               15199.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                26
    Run time :                                   12196 sec.
    Turnaround time :                            21364 sec.

The output (if any) is above this job summary.



PS:

Read file <error_23153405.log> for stderr output of this job.

